{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Price Is Right\n",
    "\n",
    "In order to examine the impacts of different cost functions on specific problems we are going to use an example from the gameshow The Price is Right. In the first round of The Price is Right, 4 contestants \"Come on down!\" to play against each other. They are given an item and asked to guess how much it costs. If the contestant goes over the price, they automatically lose. The winner is the contestant with the guess that is closest to the actual price without going over.  \n",
    "  \n",
    "So what if we want to maximize our chances of winning on The Price is Right? Well as a data scientist we might think that this is the perfect chance to train a model to make predictions for us. We need to build a model that gets as close to the correct price as possible without going over. We are going to explore how we can use cost functions to build the best model possible.  \n",
    "\n",
    "We are using data from the Amazon Toy Dataset to train and test on. With the price as our target variable, I have taken the liberty of building our exogenous variables using NLP and some basic data cleaning on a set of descriptions, reviews, and information for each product.  \n",
    "\n",
    "In this experiment we are going to use a very popular algorithm, [LightGBM from Microsoft](https://lightgbm.readthedocs.io/en/latest/). With LightGBM, we are going to explore several different functions to try to optimize our results. We are going to use the basic MAE and MSE along with some custom objective and evaluation functions of our own to see which one works best.  \n",
    "\n",
    "To evaluate our \"winnings\" on The Price is Right, we are going to calculate them as follows. If a prediction is over the true value, our winnings are 0. If our prediction is below the true value, then our winnings are equal to our prediction. For example, if an item was worth $10 and we predicted it was worth 7, we would win 7 dollars. This will encourage us to not only predict under the actual value, but still predict as close to the value as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Winnings\n",
    "First we are going to write a class to calculate the winnings our model would make from its set of predictions. It will also tell us how many of the products our model overpredicted. If our model overpredicts, our winnings are zero. If our model underpredicts, then our winnings equal our prediction.  \n",
    "\n",
    "We will also look at winnings as a percentage of each price we predict under. That will see how we fare when we don't weight more expensive items more heavily. This will give us multiple points of view for how our models are doing. It also could possibly represent someone who would rather win a big item vs. someone just interested in moving onto the second round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class price_is_right:\n",
    "    def __init__(self, truth, preds):\n",
    "        self.truths = truth\n",
    "        self.preds = preds\n",
    "        \n",
    "    def calculate_winnings(self):\n",
    "        winning_df = pd.DataFrame(zip(self.truths, self.preds), columns=['truth', 'pred'])\n",
    "        winning_df['winnings'] = 0\n",
    "        winning_df['winning_percent'] = 0\n",
    "        winning_df.loc[winning_df['pred'] <= winning_df['truth'], 'winnings'] = winning_df['pred']\n",
    "        winning_df.loc[winning_df['pred'] <= winning_df['truth'], 'winning_percent'] = winning_df['pred']/winning_df['truth']\n",
    "        self.winnings = winning_df['winnings'].sum()\n",
    "        self.winning_percent = winning_df['winning_percent'].sum()\n",
    "        self.overpredicted = winning_df[winning_df['winnings']==0].shape[0]\n",
    "        self.over_percent = self.overpredicted/len(self.preds)\n",
    "    \n",
    "    def print_results(self):\n",
    "        self.calculate_winnings()\n",
    "        print(f'OVERPREDICTED: {self.overpredicted}/{len(self.preds)}')\n",
    "        print(f'STANDARDIZED WINNINGS: {self.winning_percent}')\n",
    "        print(f'TOTAL WINNINGS: {self.winnings}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/amazon_train.csv')\n",
    "test = pd.read_csv('../data/amazon_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Let's take a quick overview of the data we are using. The price column is going to be our target. The manufacturer, seller, category, and subcategory columns have been encoded using [scikit-learns LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html). The name, info, description, and reviews columns were created with word embeddings from [spacy](https://spacy.io/usage). If you are interested in how the data was prepared feel free to check out my [other notebook](../notebooks/data_cleaning.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>price</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>average_review_rating</th>\n",
       "      <th>sellers</th>\n",
       "      <th>cat</th>\n",
       "      <th>sub_cat</th>\n",
       "      <th>name</th>\n",
       "      <th>info</th>\n",
       "      <th>desc</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>987</td>\n",
       "      <td>8.44</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>549</td>\n",
       "      <td>34</td>\n",
       "      <td>14</td>\n",
       "      <td>0.605618</td>\n",
       "      <td>-1.305930</td>\n",
       "      <td>-1.993142</td>\n",
       "      <td>1.824300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1574</td>\n",
       "      <td>209.99</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>699</td>\n",
       "      <td>13</td>\n",
       "      <td>116</td>\n",
       "      <td>3.528920</td>\n",
       "      <td>-1.269589</td>\n",
       "      <td>-1.730483</td>\n",
       "      <td>-4.421397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1663</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>81</td>\n",
       "      <td>14</td>\n",
       "      <td>46</td>\n",
       "      <td>-3.511664</td>\n",
       "      <td>-0.837028</td>\n",
       "      <td>-2.297708</td>\n",
       "      <td>-3.458916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2342</td>\n",
       "      <td>19.99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>160</td>\n",
       "      <td>7</td>\n",
       "      <td>140</td>\n",
       "      <td>0.836040</td>\n",
       "      <td>-1.123865</td>\n",
       "      <td>-2.583533</td>\n",
       "      <td>-2.375369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2105</td>\n",
       "      <td>1.49</td>\n",
       "      <td>77.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>387</td>\n",
       "      <td>0</td>\n",
       "      <td>97</td>\n",
       "      <td>1.504639</td>\n",
       "      <td>5.455936</td>\n",
       "      <td>-1.236510</td>\n",
       "      <td>-2.221026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   manufacturer   price  number_of_reviews  average_review_rating  sellers  \\\n",
       "0           987    8.44                2.0                    5.0      549   \n",
       "1          1574  209.99                2.0                    4.0      699   \n",
       "2          1663    4.00                3.0                    5.0       81   \n",
       "3          2342   19.99                1.0                    5.0      160   \n",
       "4          2105    1.49               77.0                    4.5      387   \n",
       "\n",
       "   cat  sub_cat      name      info      desc   reviews  \n",
       "0   34       14  0.605618 -1.305930 -1.993142  1.824300  \n",
       "1   13      116  3.528920 -1.269589 -1.730483 -4.421397  \n",
       "2   14       46 -3.511664 -0.837028 -2.297708 -3.458916  \n",
       "3    7      140  0.836040 -1.123865 -2.583533 -2.375369  \n",
       "4    0       97  1.504639  5.455936 -1.236510 -2.221026  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to split our test data into a validation and test set so that we have a validation set to help tune our model with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "test, val = train_test_split(test, test_size=.25, random_state=0)\n",
    "x_train = train.drop('price', 1)\n",
    "y_train = train['price']\n",
    "x_val = val.drop('price', 1)\n",
    "y_val = val['price']\n",
    "x_test = test.drop('price', 1)\n",
    "y_test = test['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM\n",
    "Here is a quick helper function to help us run experiments and get the evaluations back. We will run them with all the same hyperparameters and with a random state set to try to make it as fair as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(x_train, y_train, x_val, y_val, x_test, y_test, obj, eval_met):\n",
    "    lgb = LGBMRegressor(objective = obj,\n",
    "                    subsample = 0.1,\n",
    "                     random_state = 0,\n",
    "                     num_leaves = 120,\n",
    "                     n_estimators = 10000,\n",
    "                     min_split_gain = 0.06999,\n",
    "                     min_data_in_leaf = 12,\n",
    "                     max_depth = 56,\n",
    "                     learning_rate = 0.005,\n",
    "                     colsample_bytree = 0.4,\n",
    "                     boosting_type = 'gbdt')\n",
    "\n",
    "    lgb.fit(x_train, y_train, eval_set=(x_val, y_val), eval_metric=eval_met, early_stopping_rounds=25, verbose=False)\n",
    "\n",
    "    preds = lgb.predict(x_test)\n",
    "    win = price_is_right(y_test, preds)\n",
    "    return win"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "Here is the meat of what we are investigating. We are looking at how different loss functions will impact how our model makes predictions. The 2 functions that we will look at that are already supported by LightGBM are the [Mean Absolute Error](https://en.wikipedia.org/wiki/Mean_absolute_error) and the [Mean Squared Error](https://en.wikipedia.org/wiki/Mean_squared_error). We are also going to write a few of our own functions to try to get the most out of our model.  \n",
    "\n",
    "With LightGBM there are 2 different types of functions that we need to write. The first one is an objective function for training. This function needs to be compatible with the [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) used to optimize the model. LightGBM requires objective functions to return both the first and second derivative of the cost function.  \n",
    "\n",
    "The second type of function that we can use with LightGBM is an evaluation function. This function is how we will optimize our models parameters with our validation data during training. So while updating weights based on the objective function, the model will run until the score of our evaluation function stops improving. \n",
    "\n",
    "Most of these functions are going to be some variation of the MAE or MSE cost functions mentioned above. The difference is that we are going to add extra penalties if the model predicts above the true value. You will see that we are going to run these experiments several times with different penalties to see how the model is impacted.  \n",
    "\n",
    "The last custom function we are going to look at is going to be very specific to our problem at hand. We are writing a function where the loss approximates our price is right evaluation. So it is trying to maximize our winnings from that specific game.   \n",
    "\n",
    "So now let's see how the various models perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty = 3\n",
    "def overprediction_penalty_objective(truth, pred):\n",
    "    err = (truth - pred).astype(\"float\")\n",
    "    grad = np.where(err<0, -2*penalty*err, -2*err)\n",
    "    hess = np.where(err<0, 2*penalty, 2)\n",
    "    return grad, hess\n",
    "\n",
    "def overprediction_penalty_squared(truth, pred):\n",
    "    err = (truth - pred).astype(\"float\")\n",
    "    loss = np.where(err<0, (err**2)*penalty, err**2) \n",
    "    return \"overprediction_squared_penalty\", np.mean(loss), False\n",
    "\n",
    "def overprediction_penalty_absolute(truth, pred):\n",
    "    err = (truth - pred).astype(\"float\")\n",
    "    loss = np.where(err<0, abs(err)*penalty, abs(err)) \n",
    "    return \"overprediction_squared_penalty\", np.mean(loss), False\n",
    "\n",
    "def overprediction_penalty_pr(truth, pred):\n",
    "    err = (truth - pred).astype(\"float\")\n",
    "    loss = np.where(err<0, -pred, 0)\n",
    "    return \"custom_penalty\", np.mean(loss), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_names = ['MAE', 'MSE', 'Asymmetric MAE Custom', 'Asymmetric MSE Custom', 'Asymmetric Price is Right Eval']\n",
    "total_results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = run_experiment(x_train, y_train, x_val, y_val, x_test, y_test, 'regression_l1', 'regression_l1')\n",
    "l2 = run_experiment(x_train, y_train, x_val, y_val, x_test, y_test, 'regression_l2', 'regression_l1')\n",
    "l1_custom = run_experiment(x_train, y_train, x_val, y_val, x_test, y_test,\n",
    "                           overprediction_penalty_objective, overprediction_penalty_absolute)\n",
    "l2_custom = run_experiment(x_train, y_train, x_val, y_val, x_test, y_test,\n",
    "                           overprediction_penalty_objective, overprediction_penalty_squared)\n",
    "pr_custom = run_experiment(x_train, y_train, x_val, y_val, x_test, y_test,\n",
    "                           overprediction_penalty_objective, overprediction_penalty_custom)\n",
    "\n",
    "temp = pd.DataFrame()\n",
    "all_winnings = []\n",
    "all_std_winnings = []\n",
    "overpreds = []\n",
    "results = [l1, l2, l1_custom, l2_custom, pr_custom]\n",
    "for res, name in zip(results, res_names) :\n",
    "    res.calculate_winnings()\n",
    "    all_winnings.append(res.winnings)\n",
    "    all_std_winnings.append(res.winning_percent)\n",
    "    overpreds.append(res.over_percent)\n",
    "temp['Cost'] = res_names\n",
    "temp['Winnings'] = all_winnings\n",
    "temp['Standardized_Winnings'] = all_std_winnings\n",
    "temp['% OverPredicted'] = overpreds\n",
    "temp['Penalty'] = penalty\n",
    "total_results = pd.concat([total_results, temp], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty = 5\n",
    "def overprediction_penalty_objective(truth, pred):\n",
    "    err = (truth - pred).astype(\"float\")\n",
    "    grad = np.where(err<0, -2*penalty*err, -2*err)\n",
    "    hess = np.where(err<0, 2*penalty, 2)\n",
    "    return grad, hess\n",
    "\n",
    "def overprediction_penalty_squared(truth, pred):\n",
    "    err = (truth - pred).astype(\"float\")\n",
    "    loss = np.where(err<0, (err**2)*penalty, err**2) \n",
    "    return \"overprediction_squared_penalty\", np.mean(loss), False\n",
    "\n",
    "def overprediction_penalty_absolute(truth, pred):\n",
    "    err = (truth - pred).astype(\"float\")\n",
    "    loss = np.where(err<0, abs(err)*penalty, abs(err)) \n",
    "    return \"overprediction_squared_penalty\", np.mean(loss), False\n",
    "\n",
    "def overprediction_penalty_custom(truth, pred):\n",
    "    err = (truth - pred).astype(\"float\")\n",
    "    loss = np.where(err<0, -pred, 0)\n",
    "    return \"custom_penalty\", np.mean(loss), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = run_experiment(x_train, y_train, x_val, y_val, x_test, y_test, 'regression_l1', 'regression_l1')\n",
    "l2 = run_experiment(x_train, y_train, x_val, y_val, x_test, y_test, 'regression_l2', 'regression_l1')\n",
    "l1_custom = run_experiment(x_train, y_train, x_val, y_val, x_test, y_test,\n",
    "                           overprediction_penalty_objective, overprediction_penalty_absolute)\n",
    "l2_custom = run_experiment(x_train, y_train, x_val, y_val, x_test, y_test,\n",
    "                           overprediction_penalty_objective, overprediction_penalty_squared)\n",
    "pr_custom = run_experiment(x_train, y_train, x_val, y_val, x_test, y_test,\n",
    "                           overprediction_penalty_objective, overprediction_penalty_custom)\n",
    "\n",
    "temp = pd.DataFrame()\n",
    "all_winnings = []\n",
    "all_std_winnings = []\n",
    "overpreds = []\n",
    "results = [l1, l2, l1_custom, l2_custom, pr_custom]\n",
    "for res, name in zip(results, res_names) :\n",
    "    res.calculate_winnings()\n",
    "    all_winnings.append(res.winnings)\n",
    "    all_std_winnings.append(res.winning_percent)\n",
    "    overpreds.append(res.over_percent)\n",
    "temp['Cost'] = res_names\n",
    "temp['Winnings'] = all_winnings\n",
    "temp['Standardized_Winnings'] = all_std_winnings\n",
    "temp['% OverPredicted'] = overpreds\n",
    "temp['Penalty'] = penalty\n",
    "total_results = pd.concat([total_results, temp], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty = 10\n",
    "def overprediction_penalty_objective(truth, pred):\n",
    "    err = (truth - pred).astype(\"float\")\n",
    "    grad = np.where(err<0, -2*penalty*err, -2*err)\n",
    "    hess = np.where(err<0, 2*penalty, 2)\n",
    "    return grad, hess\n",
    "\n",
    "def overprediction_penalty_squared(truth, pred):\n",
    "    err = (truth - pred).astype(\"float\")\n",
    "    loss = np.where(err<0, (err**2)*penalty, err**2) \n",
    "    return \"overprediction_squared_penalty\", np.mean(loss), False\n",
    "\n",
    "def overprediction_penalty_absolute(truth, pred):\n",
    "    err = (truth - pred).astype(\"float\")\n",
    "    loss = np.where(err<0, abs(err)*penalty, abs(err)) \n",
    "    return \"overprediction_squared_penalty\", np.mean(loss), False\n",
    "\n",
    "def overprediction_penalty_custom(truth, pred):\n",
    "    err = (truth - pred).astype(\"float\")\n",
    "    loss = np.where(err<0, -pred, 0)\n",
    "    return \"custom_penalty\", np.mean(loss), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = run_experiment(x_train, y_train, x_val, y_val, x_test, y_test, 'regression_l1', 'regression_l1')\n",
    "l2 = run_experiment(x_train, y_train, x_val, y_val, x_test, y_test, 'regression_l2', 'regression_l1')\n",
    "l1_custom = run_experiment(x_train, y_train, x_val, y_val, x_test, y_test,\n",
    "                           overprediction_penalty_objective, overprediction_penalty_absolute)\n",
    "l2_custom = run_experiment(x_train, y_train, x_val, y_val, x_test, y_test,\n",
    "                           overprediction_penalty_objective, overprediction_penalty_squared)\n",
    "pr_custom = run_experiment(x_train, y_train, x_val, y_val, x_test, y_test,\n",
    "                           overprediction_penalty_objective, overprediction_penalty_custom)\n",
    "\n",
    "temp = pd.DataFrame()\n",
    "all_winnings = []\n",
    "all_std_winnings = []\n",
    "overpreds = []\n",
    "results = [l1, l2, l1_custom, l2_custom, pr_custom]\n",
    "for res, name in zip(results, res_names) :\n",
    "    res.calculate_winnings()\n",
    "    all_winnings.append(res.winnings)\n",
    "    all_std_winnings.append(res.winning_percent)\n",
    "    overpreds.append(res.over_percent)\n",
    "temp['Cost'] = res_names\n",
    "temp['Winnings'] = all_winnings\n",
    "temp['Standardized_Winnings'] = all_std_winnings\n",
    "temp['% OverPredicted'] = overpreds\n",
    "temp['Penalty'] = penalty\n",
    "total_results = pd.concat([total_results, temp], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty = 12\n",
    "def overprediction_penalty_objective(truth, pred):\n",
    "    err = (truth - pred).astype(\"float\")\n",
    "    grad = np.where(err<0, -2*penalty*err, -2*err)\n",
    "    hess = np.where(err<0, 2*penalty, 2)\n",
    "    return grad, hess\n",
    "\n",
    "def overprediction_penalty_squared(truth, pred):\n",
    "    err = (truth - pred).astype(\"float\")\n",
    "    loss = np.where(err<0, (err**2)*penalty, err**2) \n",
    "    return \"overprediction_squared_penalty\", np.mean(loss), False\n",
    "\n",
    "def overprediction_penalty_absolute(truth, pred):\n",
    "    err = (truth - pred).astype(\"float\")\n",
    "    loss = np.where(err<0, abs(err)*penalty, abs(err)) \n",
    "    return \"overprediction_squared_penalty\", np.mean(loss), False\n",
    "\n",
    "def overprediction_penalty_custom(truth, pred):\n",
    "    err = (truth - pred).astype(\"float\")\n",
    "    loss = np.where(err<0, -pred, 0)\n",
    "    return \"custom_penalty\", np.mean(loss), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = run_experiment(x_train, y_train, x_val, y_val, x_test, y_test, 'regression_l1', 'regression_l1')\n",
    "l2 = run_experiment(x_train, y_train, x_val, y_val, x_test, y_test, 'regression_l2', 'regression_l1')\n",
    "l1_custom = run_experiment(x_train, y_train, x_val, y_val, x_test, y_test,\n",
    "                           overprediction_penalty_objective, overprediction_penalty_absolute)\n",
    "l2_custom = run_experiment(x_train, y_train, x_val, y_val, x_test, y_test,\n",
    "                           overprediction_penalty_objective, overprediction_penalty_squared)\n",
    "pr_custom = run_experiment(x_train, y_train, x_val, y_val, x_test, y_test,\n",
    "                           overprediction_penalty_objective, overprediction_penalty_custom)\n",
    "\n",
    "temp = pd.DataFrame()\n",
    "all_winnings = []\n",
    "all_std_winnings = []\n",
    "overpreds = []\n",
    "results = [l1, l2, l1_custom, l2_custom, pr_custom]\n",
    "for res, name in zip(results, res_names) :\n",
    "    res.calculate_winnings()\n",
    "    all_winnings.append(res.winnings)\n",
    "    all_std_winnings.append(res.winning_percent)\n",
    "    overpreds.append(res.over_percent)\n",
    "temp['Cost'] = res_names\n",
    "temp['Winnings'] = all_winnings\n",
    "temp['Standardized_Winnings'] = all_std_winnings\n",
    "temp['% OverPredicted'] = overpreds\n",
    "temp['Penalty'] = penalty\n",
    "total_results = pd.concat([total_results, temp], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cost</th>\n",
       "      <th>Winnings</th>\n",
       "      <th>Standardized_Winnings</th>\n",
       "      <th>% OverPredicted</th>\n",
       "      <th>Penalty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Asymmetric MAE Custom</td>\n",
       "      <td>9620.347383</td>\n",
       "      <td>670.328990</td>\n",
       "      <td>0.214249</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Asymmetric MSE Custom</td>\n",
       "      <td>12109.453288</td>\n",
       "      <td>657.519941</td>\n",
       "      <td>0.357774</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Asymmetric Price is Right Eval</td>\n",
       "      <td>13040.864275</td>\n",
       "      <td>591.461058</td>\n",
       "      <td>0.456058</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>MAE</td>\n",
       "      <td>13106.620179</td>\n",
       "      <td>518.973499</td>\n",
       "      <td>0.557982</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>MSE</td>\n",
       "      <td>12083.902792</td>\n",
       "      <td>332.002559</td>\n",
       "      <td>0.731669</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Cost      Winnings  Standardized_Winnings  \\\n",
       "2           Asymmetric MAE Custom   9620.347383             670.328990   \n",
       "3           Asymmetric MSE Custom  12109.453288             657.519941   \n",
       "4  Asymmetric Price is Right Eval  13040.864275             591.461058   \n",
       "0                             MAE  13106.620179             518.973499   \n",
       "1                             MSE  12083.902792             332.002559   \n",
       "\n",
       "   % OverPredicted  Penalty  \n",
       "2         0.214249        5  \n",
       "3         0.357774       12  \n",
       "4         0.456058       12  \n",
       "0         0.557982       12  \n",
       "1         0.731669       10  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_winnings = total_results.sort_values(by='Winnings', ascending=False).drop_duplicates(subset=['Cost'])\n",
    "best_std_winnings = total_results.sort_values(by='Standardized_Winnings', ascending=False).drop_duplicates(subset=['Cost'])\n",
    "best_std_winnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cost</th>\n",
       "      <th>Winnings</th>\n",
       "      <th>Standardized_Winnings</th>\n",
       "      <th>% OverPredicted</th>\n",
       "      <th>Penalty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Asymmetric Price is Right Eval</td>\n",
       "      <td>13298.838315</td>\n",
       "      <td>557.408396</td>\n",
       "      <td>0.486739</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Asymmetric MSE Custom</td>\n",
       "      <td>13213.585046</td>\n",
       "      <td>515.005578</td>\n",
       "      <td>0.549142</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>MAE</td>\n",
       "      <td>13106.620179</td>\n",
       "      <td>518.973499</td>\n",
       "      <td>0.557982</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>MSE</td>\n",
       "      <td>12083.902792</td>\n",
       "      <td>332.002559</td>\n",
       "      <td>0.731669</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Asymmetric MAE Custom</td>\n",
       "      <td>11387.717411</td>\n",
       "      <td>669.309173</td>\n",
       "      <td>0.315133</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Cost      Winnings  Standardized_Winnings  \\\n",
       "4  Asymmetric Price is Right Eval  13298.838315             557.408396   \n",
       "3           Asymmetric MSE Custom  13213.585046             515.005578   \n",
       "0                             MAE  13106.620179             518.973499   \n",
       "1                             MSE  12083.902792             332.002559   \n",
       "2           Asymmetric MAE Custom  11387.717411             669.309173   \n",
       "\n",
       "   % OverPredicted  Penalty  \n",
       "4         0.486739       10  \n",
       "3         0.549142        3  \n",
       "0         0.557982        3  \n",
       "1         0.731669        5  \n",
       "2         0.315133        3  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_winnings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
